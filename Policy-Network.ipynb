{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Reinforcement Learning in Tensorflow Part 2: Policy Gradient Method\n",
    "This tutorial contains a simple example of how to build a policy-gradient based agent that can solve the CartPole problem. For more information, see this [Medium post](https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724#.mtwpvfi8b).\n",
    "\n",
    "For more Reinforcement Learning algorithms, including DQN and Model-based learning in Tensorflow, see my Github repo, [DeepRL-Agents](https://github.com/awjuliani/DeepRL-Agents). \n",
    "\n",
    "Parts of this tutorial are based on code by [Andrej Karpathy](https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5) and [korymath](https://gym.openai.com/evaluations/eval_a0aVJrGSyW892vBM04HQA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "try:\n",
    "    xrange = xrange\n",
    "except:\n",
    "    xrange = range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the CartPole Environment\n",
    "If you don't already have the OpenAI gym installed, use  `pip install gym` to grab it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we try running the environment with random actions? How well do we do? (Hint: not so well.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'flip'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-67da73623287>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mreward_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mrandom_episodes\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mreward_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, close)\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36m_render\u001b[1;34m(self, mode, close)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_close\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, close)\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\mountain_car.py\u001b[0m in \u001b[0;36m_render\u001b[1;34m(self, mode, close)\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcartrans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, return_rgb_array)\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monetime_geoms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\u001b[0m in \u001b[0;36mflip\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_mouse_cursor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mset_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'flip'"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "random_episodes = 0\n",
    "reward_sum = 0\n",
    "while random_episodes < 10:\n",
    "    env.render()\n",
    "    observation, reward, done, _ = env.step(np.random.randint(0,2))\n",
    "    reward_sum += reward\n",
    "    if done:\n",
    "        random_episodes += 1\n",
    "        print(\"Reward for this episode was:\",reward_sum)\n",
    "        reward_sum = 0\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the task is to achieve a reward of 200 per episode. For every step the agent keeps the pole in the air, the agent recieves a +1 reward. By randomly choosing actions, our reward for each episode is only a couple dozen. Let's make that better with RL!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up our Neural Network agent\n",
    "This time we will be using a Policy neural network that takes observations, passes them through a single hidden layer, and then produces a probability of choosing a left/right movement. To learn more about this network, see [Andrej Karpathy's blog on Policy Gradient networks](http://karpathy.github.io/2016/05/31/rl/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "H = 10 # number of hidden layer neurons\n",
    "batch_size = 5 # every how many episodes to do a param update?\n",
    "learning_rate = 1e-2 # feel free to play with this to train faster or more stably.\n",
    "gamma = 0.8 # discount factor for reward\n",
    "\n",
    "D = 2 # input dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#This defines the network as it goes from taking an observation of the environment to \n",
    "#giving a probability of chosing to the action of moving left or right.\n",
    "observations = tf.placeholder(tf.float32, [None,D] , name=\"input_x\")\n",
    "W1 = tf.get_variable(\"W1\", shape=[D, H],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "layer1 = tf.nn.relu(tf.matmul(observations,W1))\n",
    "W2 = tf.get_variable(\"W2\", shape=[H, 1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "score = tf.matmul(layer1,W2)\n",
    "probability = tf.nn.sigmoid(score)\n",
    "\n",
    "#From here we define the parts of the network needed for learning a good policy.\n",
    "tvars = tf.trainable_variables()\n",
    "input_y = tf.placeholder(tf.float32,[None,1], name=\"input_y\")\n",
    "advantages = tf.placeholder(tf.float32,name=\"reward_signal\")\n",
    "\n",
    "# The loss function. This sends the weights in the direction of making actions \n",
    "# that gave good advantage (reward over time) more likely, and actions that didn't less likely.\n",
    "loglik = tf.log(input_y*(input_y - probability) + (1 - input_y)*(input_y + probability))\n",
    "loss = -tf.reduce_mean(loglik * advantages) \n",
    "newGrads = tf.gradients(loss,tvars)\n",
    "\n",
    "# Once we have collected a series of gradients from multiple episodes, we apply them.\n",
    "# We don't just apply gradeients after every episode in order to account for noise in the reward signal.\n",
    "adam = tf.train.AdamOptimizer(learning_rate=learning_rate) # Our optimizer\n",
    "W1Grad = tf.placeholder(tf.float32,name=\"batch_grad1\") # Placeholders to send the final gradients through when we update.\n",
    "W2Grad = tf.placeholder(tf.float32,name=\"batch_grad2\")\n",
    "batchGrad = [W1Grad,W2Grad]\n",
    "updateGrads = adam.apply_gradients(zip(batchGrad,tvars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage function\n",
    "This function allows us to weigh the rewards our agent recieves. In the context of the Cart-Pole task, we want actions that kept the pole in the air a long time to have a large reward, and actions that contributed to the pole falling to have a decreased or negative reward. We do this by weighing the rewards from the end of the episode, with actions at the end being seen as negative, since they likely contributed to the pole falling, and the episode ending. Likewise, early actions are seen as more positive, since they weren't responsible for the pole falling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Agent and Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we run the neural network agent, and have it act in the CartPole environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5. Average reward for episode -160.000000.  Total average reward -160.000000.\n",
      "Episode 10. Average reward for episode -200.000000.  Total average reward -161.000000.\n",
      "Episode 15. Average reward for episode -200.000000.  Total average reward -161.000000.\n",
      "Episode 20. Average reward for episode -200.000000.  Total average reward -162.000000.\n",
      "Episode 25. Average reward for episode -200.000000.  Total average reward -162.000000.\n",
      "Episode 30. Average reward for episode -200.000000.  Total average reward -162.000000.\n",
      "Episode 35. Average reward for episode -200.000000.  Total average reward -163.000000.\n",
      "Episode 40. Average reward for episode -200.000000.  Total average reward -163.000000.\n",
      "Episode 45. Average reward for episode -200.000000.  Total average reward -164.000000.\n",
      "Episode 50. Average reward for episode -200.000000.  Total average reward -164.000000.\n",
      "Episode 55. Average reward for episode -200.000000.  Total average reward -164.000000.\n",
      "Episode 60. Average reward for episode -200.000000.  Total average reward -165.000000.\n",
      "Episode 65. Average reward for episode -200.000000.  Total average reward -165.000000.\n",
      "Episode 70. Average reward for episode -200.000000.  Total average reward -165.000000.\n",
      "Episode 75. Average reward for episode -200.000000.  Total average reward -166.000000.\n",
      "Episode 80. Average reward for episode -200.000000.  Total average reward -166.000000.\n",
      "Episode 85. Average reward for episode -200.000000.  Total average reward -166.000000.\n",
      "Episode 90. Average reward for episode -200.000000.  Total average reward -167.000000.\n",
      "Episode 95. Average reward for episode -200.000000.  Total average reward -167.000000.\n",
      "Episode 100. Average reward for episode -200.000000.  Total average reward -167.000000.\n",
      "Episode 105. Average reward for episode -200.000000.  Total average reward -168.000000.\n",
      "Episode 110. Average reward for episode -200.000000.  Total average reward -168.000000.\n",
      "Episode 115. Average reward for episode -200.000000.  Total average reward -168.000000.\n",
      "Episode 120. Average reward for episode -200.000000.  Total average reward -169.000000.\n",
      "Episode 125. Average reward for episode -200.000000.  Total average reward -169.000000.\n",
      "Episode 130. Average reward for episode -200.000000.  Total average reward -169.000000.\n",
      "Episode 135. Average reward for episode -200.000000.  Total average reward -170.000000.\n",
      "Episode 140. Average reward for episode -200.000000.  Total average reward -170.000000.\n",
      "Episode 145. Average reward for episode -200.000000.  Total average reward -170.000000.\n",
      "Episode 150. Average reward for episode -200.000000.  Total average reward -171.000000.\n",
      "Episode 155. Average reward for episode -200.000000.  Total average reward -171.000000.\n",
      "Episode 160. Average reward for episode -200.000000.  Total average reward -171.000000.\n",
      "Episode 165. Average reward for episode -200.000000.  Total average reward -172.000000.\n",
      "Episode 170. Average reward for episode -200.000000.  Total average reward -172.000000.\n",
      "Episode 175. Average reward for episode -200.000000.  Total average reward -172.000000.\n",
      "Episode 180. Average reward for episode -200.000000.  Total average reward -172.000000.\n",
      "Episode 185. Average reward for episode -200.000000.  Total average reward -173.000000.\n",
      "Episode 190. Average reward for episode -200.000000.  Total average reward -173.000000.\n",
      "Episode 195. Average reward for episode -200.000000.  Total average reward -173.000000.\n",
      "Episode 200. Average reward for episode -200.000000.  Total average reward -173.000000.\n",
      "Episode 205. Average reward for episode -200.000000.  Total average reward -174.000000.\n",
      "Episode 210. Average reward for episode -200.000000.  Total average reward -174.000000.\n",
      "Episode 215. Average reward for episode -200.000000.  Total average reward -174.000000.\n",
      "Episode 220. Average reward for episode -200.000000.  Total average reward -175.000000.\n",
      "Episode 225. Average reward for episode -200.000000.  Total average reward -175.000000.\n",
      "Episode 230. Average reward for episode -200.000000.  Total average reward -175.000000.\n",
      "Episode 235. Average reward for episode -200.000000.  Total average reward -175.000000.\n",
      "Episode 240. Average reward for episode -200.000000.  Total average reward -176.000000.\n",
      "Episode 245. Average reward for episode -200.000000.  Total average reward -176.000000.\n",
      "Episode 250. Average reward for episode -200.000000.  Total average reward -176.000000.\n",
      "Episode 255. Average reward for episode -200.000000.  Total average reward -176.000000.\n",
      "Episode 260. Average reward for episode -200.000000.  Total average reward -177.000000.\n",
      "Episode 265. Average reward for episode -200.000000.  Total average reward -177.000000.\n",
      "Episode 270. Average reward for episode -200.000000.  Total average reward -177.000000.\n",
      "Episode 275. Average reward for episode -200.000000.  Total average reward -177.000000.\n",
      "Episode 280. Average reward for episode -200.000000.  Total average reward -177.000000.\n",
      "Episode 285. Average reward for episode -200.000000.  Total average reward -178.000000.\n",
      "Episode 290. Average reward for episode -200.000000.  Total average reward -178.000000.\n",
      "Episode 295. Average reward for episode -200.000000.  Total average reward -178.000000.\n",
      "Episode 300. Average reward for episode -200.000000.  Total average reward -178.000000.\n",
      "Episode 305. Average reward for episode -200.000000.  Total average reward -179.000000.\n",
      "Episode 310. Average reward for episode -200.000000.  Total average reward -179.000000.\n",
      "Episode 315. Average reward for episode -200.000000.  Total average reward -179.000000.\n",
      "Episode 320. Average reward for episode -200.000000.  Total average reward -179.000000.\n",
      "Episode 325. Average reward for episode -200.000000.  Total average reward -179.000000.\n",
      "Episode 330. Average reward for episode -200.000000.  Total average reward -180.000000.\n",
      "Episode 335. Average reward for episode -200.000000.  Total average reward -180.000000.\n",
      "Episode 340. Average reward for episode -200.000000.  Total average reward -180.000000.\n",
      "Episode 345. Average reward for episode -200.000000.  Total average reward -180.000000.\n",
      "Episode 350. Average reward for episode -200.000000.  Total average reward -181.000000.\n",
      "Episode 355. Average reward for episode -200.000000.  Total average reward -181.000000.\n",
      "Episode 360. Average reward for episode -200.000000.  Total average reward -181.000000.\n",
      "Episode 365. Average reward for episode -200.000000.  Total average reward -181.000000.\n",
      "Episode 370. Average reward for episode -200.000000.  Total average reward -181.000000.\n",
      "Episode 375. Average reward for episode -200.000000.  Total average reward -181.000000.\n",
      "Episode 380. Average reward for episode -200.000000.  Total average reward -182.000000.\n",
      "Episode 385. Average reward for episode -200.000000.  Total average reward -182.000000.\n",
      "Episode 390. Average reward for episode -200.000000.  Total average reward -182.000000.\n",
      "Episode 395. Average reward for episode -200.000000.  Total average reward -182.000000.\n",
      "Episode 400. Average reward for episode -200.000000.  Total average reward -182.000000.\n",
      "Episode 405. Average reward for episode -200.000000.  Total average reward -183.000000.\n",
      "Episode 410. Average reward for episode -200.000000.  Total average reward -183.000000.\n",
      "Episode 415. Average reward for episode -200.000000.  Total average reward -183.000000.\n",
      "Episode 420. Average reward for episode -200.000000.  Total average reward -183.000000.\n",
      "Episode 425. Average reward for episode -200.000000.  Total average reward -183.000000.\n",
      "Episode 430. Average reward for episode -200.000000.  Total average reward -183.000000.\n",
      "Episode 435. Average reward for episode -200.000000.  Total average reward -184.000000.\n",
      "Episode 440. Average reward for episode -200.000000.  Total average reward -184.000000.\n",
      "Episode 445. Average reward for episode -200.000000.  Total average reward -184.000000.\n",
      "Episode 450. Average reward for episode -200.000000.  Total average reward -184.000000.\n",
      "Episode 455. Average reward for episode -200.000000.  Total average reward -184.000000.\n",
      "Episode 460. Average reward for episode -200.000000.  Total average reward -184.000000.\n",
      "Episode 465. Average reward for episode -200.000000.  Total average reward -185.000000.\n",
      "Episode 470. Average reward for episode -200.000000.  Total average reward -185.000000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 475. Average reward for episode -200.000000.  Total average reward -185.000000.\n",
      "Episode 480. Average reward for episode -200.000000.  Total average reward -185.000000.\n",
      "Episode 485. Average reward for episode -200.000000.  Total average reward -185.000000.\n",
      "Episode 490. Average reward for episode -200.000000.  Total average reward -185.000000.\n",
      "Episode 495. Average reward for episode -200.000000.  Total average reward -186.000000.\n",
      "Episode 500. Average reward for episode -200.000000.  Total average reward -186.000000.\n",
      "Episode 505. Average reward for episode -200.000000.  Total average reward -186.000000.\n",
      "Episode 510. Average reward for episode -200.000000.  Total average reward -186.000000.\n",
      "Episode 515. Average reward for episode -200.000000.  Total average reward -186.000000.\n",
      "Episode 520. Average reward for episode -200.000000.  Total average reward -186.000000.\n",
      "Episode 525. Average reward for episode -200.000000.  Total average reward -186.000000.\n",
      "Episode 530. Average reward for episode -200.000000.  Total average reward -187.000000.\n",
      "Episode 535. Average reward for episode -200.000000.  Total average reward -187.000000.\n",
      "Episode 540. Average reward for episode -200.000000.  Total average reward -187.000000.\n",
      "Episode 545. Average reward for episode -200.000000.  Total average reward -187.000000.\n",
      "Episode 550. Average reward for episode -200.000000.  Total average reward -187.000000.\n",
      "Episode 555. Average reward for episode -200.000000.  Total average reward -187.000000.\n",
      "Episode 560. Average reward for episode -200.000000.  Total average reward -187.000000.\n",
      "Episode 565. Average reward for episode -200.000000.  Total average reward -188.000000.\n",
      "Episode 570. Average reward for episode -200.000000.  Total average reward -188.000000.\n",
      "Episode 575. Average reward for episode -200.000000.  Total average reward -188.000000.\n",
      "Episode 580. Average reward for episode -200.000000.  Total average reward -188.000000.\n",
      "Episode 585. Average reward for episode -200.000000.  Total average reward -188.000000.\n",
      "Episode 590. Average reward for episode -200.000000.  Total average reward -188.000000.\n",
      "Episode 595. Average reward for episode -200.000000.  Total average reward -188.000000.\n",
      "Episode 600. Average reward for episode -200.000000.  Total average reward -188.000000.\n",
      "Episode 605. Average reward for episode -200.000000.  Total average reward -189.000000.\n",
      "Episode 610. Average reward for episode -200.000000.  Total average reward -189.000000.\n",
      "Episode 615. Average reward for episode -200.000000.  Total average reward -189.000000.\n",
      "Episode 620. Average reward for episode -200.000000.  Total average reward -189.000000.\n",
      "Episode 625. Average reward for episode -200.000000.  Total average reward -189.000000.\n",
      "Episode 630. Average reward for episode -200.000000.  Total average reward -189.000000.\n",
      "Episode 635. Average reward for episode -200.000000.  Total average reward -189.000000.\n",
      "Episode 640. Average reward for episode -200.000000.  Total average reward -189.000000.\n",
      "Episode 645. Average reward for episode -200.000000.  Total average reward -189.000000.\n",
      "Episode 650. Average reward for episode -200.000000.  Total average reward -190.000000.\n",
      "Episode 655. Average reward for episode -200.000000.  Total average reward -190.000000.\n",
      "Episode 660. Average reward for episode -200.000000.  Total average reward -190.000000.\n",
      "Episode 665. Average reward for episode -200.000000.  Total average reward -190.000000.\n",
      "Episode 670. Average reward for episode -200.000000.  Total average reward -190.000000.\n",
      "Episode 675. Average reward for episode -200.000000.  Total average reward -190.000000.\n",
      "Episode 680. Average reward for episode -200.000000.  Total average reward -190.000000.\n",
      "Episode 685. Average reward for episode -200.000000.  Total average reward -190.000000.\n",
      "Episode 690. Average reward for episode -200.000000.  Total average reward -190.000000.\n",
      "Episode 695. Average reward for episode -200.000000.  Total average reward -191.000000.\n",
      "Episode 700. Average reward for episode -200.000000.  Total average reward -191.000000.\n",
      "Episode 705. Average reward for episode -200.000000.  Total average reward -191.000000.\n",
      "Episode 710. Average reward for episode -200.000000.  Total average reward -191.000000.\n",
      "Episode 715. Average reward for episode -200.000000.  Total average reward -191.000000.\n",
      "Episode 720. Average reward for episode -200.000000.  Total average reward -191.000000.\n",
      "Episode 725. Average reward for episode -200.000000.  Total average reward -191.000000.\n",
      "Episode 730. Average reward for episode -200.000000.  Total average reward -191.000000.\n",
      "Episode 735. Average reward for episode -200.000000.  Total average reward -191.000000.\n",
      "Episode 740. Average reward for episode -200.000000.  Total average reward -191.000000.\n",
      "Episode 745. Average reward for episode -200.000000.  Total average reward -191.000000.\n",
      "Episode 750. Average reward for episode -200.000000.  Total average reward -192.000000.\n",
      "Episode 755. Average reward for episode -200.000000.  Total average reward -192.000000.\n",
      "Episode 760. Average reward for episode -200.000000.  Total average reward -192.000000.\n",
      "Episode 765. Average reward for episode -200.000000.  Total average reward -192.000000.\n",
      "Episode 770. Average reward for episode -200.000000.  Total average reward -192.000000.\n",
      "Episode 775. Average reward for episode -200.000000.  Total average reward -192.000000.\n",
      "Episode 780. Average reward for episode -200.000000.  Total average reward -192.000000.\n",
      "Episode 785. Average reward for episode -200.000000.  Total average reward -192.000000.\n",
      "Episode 790. Average reward for episode -200.000000.  Total average reward -192.000000.\n",
      "Episode 795. Average reward for episode -200.000000.  Total average reward -192.000000.\n",
      "Episode 800. Average reward for episode -200.000000.  Total average reward -192.000000.\n",
      "Episode 805. Average reward for episode -200.000000.  Total average reward -192.000000.\n",
      "Episode 810. Average reward for episode -200.000000.  Total average reward -193.000000.\n",
      "Episode 815. Average reward for episode -200.000000.  Total average reward -193.000000.\n",
      "Episode 820. Average reward for episode -200.000000.  Total average reward -193.000000.\n",
      "Episode 825. Average reward for episode -200.000000.  Total average reward -193.000000.\n",
      "Episode 830. Average reward for episode -200.000000.  Total average reward -193.000000.\n",
      "Episode 835. Average reward for episode -200.000000.  Total average reward -193.000000.\n",
      "Episode 840. Average reward for episode -200.000000.  Total average reward -193.000000.\n",
      "Episode 845. Average reward for episode -200.000000.  Total average reward -193.000000.\n",
      "Episode 850. Average reward for episode -200.000000.  Total average reward -193.000000.\n",
      "Episode 855. Average reward for episode -200.000000.  Total average reward -193.000000.\n",
      "Episode 860. Average reward for episode -200.000000.  Total average reward -193.000000.\n",
      "Episode 865. Average reward for episode -200.000000.  Total average reward -193.000000.\n",
      "Episode 870. Average reward for episode -200.000000.  Total average reward -193.000000.\n",
      "Episode 875. Average reward for episode -200.000000.  Total average reward -194.000000.\n",
      "Episode 880. Average reward for episode -200.000000.  Total average reward -194.000000.\n",
      "Episode 885. Average reward for episode -200.000000.  Total average reward -194.000000.\n",
      "Episode 890. Average reward for episode -200.000000.  Total average reward -194.000000.\n",
      "Episode 895. Average reward for episode -200.000000.  Total average reward -194.000000.\n",
      "Episode 900. Average reward for episode -200.000000.  Total average reward -194.000000.\n",
      "Episode 905. Average reward for episode -200.000000.  Total average reward -194.000000.\n",
      "Episode 910. Average reward for episode -200.000000.  Total average reward -194.000000.\n",
      "Episode 915. Average reward for episode -200.000000.  Total average reward -194.000000.\n",
      "Episode 920. Average reward for episode -200.000000.  Total average reward -194.000000.\n",
      "Episode 925. Average reward for episode -200.000000.  Total average reward -194.000000.\n",
      "Episode 930. Average reward for episode -200.000000.  Total average reward -194.000000.\n",
      "Episode 935. Average reward for episode -200.000000.  Total average reward -194.000000.\n",
      "Episode 940. Average reward for episode -200.000000.  Total average reward -194.000000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 945. Average reward for episode -200.000000.  Total average reward -194.000000.\n",
      "Episode 950. Average reward for episode -200.000000.  Total average reward -195.000000.\n",
      "Episode 955. Average reward for episode -200.000000.  Total average reward -195.000000.\n",
      "Episode 960. Average reward for episode -200.000000.  Total average reward -195.000000.\n",
      "Episode 965. Average reward for episode -200.000000.  Total average reward -195.000000.\n",
      "Episode 970. Average reward for episode -200.000000.  Total average reward -195.000000.\n",
      "Episode 975. Average reward for episode -200.000000.  Total average reward -195.000000.\n",
      "Episode 980. Average reward for episode -200.000000.  Total average reward -195.000000.\n",
      "Episode 985. Average reward for episode -200.000000.  Total average reward -195.000000.\n",
      "Episode 990. Average reward for episode -200.000000.  Total average reward -195.000000.\n",
      "Episode 995. Average reward for episode -200.000000.  Total average reward -195.000000.\n",
      "Episode 1000. Average reward for episode -200.000000.  Total average reward -195.000000.\n",
      "Episode 1005. Average reward for episode -200.000000.  Total average reward -195.000000.\n",
      "Episode 1010. Average reward for episode -200.000000.  Total average reward -195.000000.\n",
      "Episode 1015. Average reward for episode -200.000000.  Total average reward -195.000000.\n",
      "Episode 1020. Average reward for episode -200.000000.  Total average reward -195.000000.\n",
      "Episode 1025. Average reward for episode -200.000000.  Total average reward -195.000000.\n",
      "Episode 1030. Average reward for episode -200.000000.  Total average reward -195.000000.\n",
      "Episode 1035. Average reward for episode -200.000000.  Total average reward -195.000000.\n",
      "Episode 1040. Average reward for episode -200.000000.  Total average reward -196.000000.\n",
      "Episode 1045. Average reward for episode -200.000000.  Total average reward -196.000000.\n",
      "Episode 1050. Average reward for episode -200.000000.  Total average reward -196.000000.\n",
      "Episode 1055. Average reward for episode -200.000000.  Total average reward -196.000000.\n",
      "Episode 1060. Average reward for episode -200.000000.  Total average reward -196.000000.\n",
      "Episode 1065. Average reward for episode -200.000000.  Total average reward -196.000000.\n",
      "Episode 1070. Average reward for episode -200.000000.  Total average reward -196.000000.\n",
      "Episode 1075. Average reward for episode -200.000000.  Total average reward -196.000000.\n",
      "Episode 1080. Average reward for episode -200.000000.  Total average reward -196.000000.\n",
      "Episode 1085. Average reward for episode -200.000000.  Total average reward -196.000000.\n",
      "Episode 1090. Average reward for episode -200.000000.  Total average reward -196.000000.\n",
      "Episode 1095. Average reward for episode -200.000000.  Total average reward -196.000000.\n",
      "Episode 1100. Average reward for episode -200.000000.  Total average reward -196.000000.\n",
      "Episode 1105. Average reward for episode -200.000000.  Total average reward -196.000000.\n",
      "Episode 1110. Average reward for episode -200.000000.  Total average reward -196.000000.\n",
      "Episode 1115. Average reward for episode -200.000000.  Total average reward -196.000000.\n",
      "Episode 1120. Average reward for episode -200.000000.  Total average reward -196.000000.\n",
      "Episode 1125. Average reward for episode -200.000000.  Total average reward -196.000000.\n",
      "Episode 1130. Average reward for episode -200.000000.  Total average reward -196.000000.\n",
      "Episode 1135. Average reward for episode -200.000000.  Total average reward -196.000000.\n",
      "Episode 1140. Average reward for episode -200.000000.  Total average reward -196.000000.\n",
      "Episode 1145. Average reward for episode -200.000000.  Total average reward -196.000000.\n",
      "Episode 1150. Average reward for episode -200.000000.  Total average reward -196.000000.\n",
      "Episode 1155. Average reward for episode -200.000000.  Total average reward -197.000000.\n",
      "Episode 1160. Average reward for episode -200.000000.  Total average reward -197.000000.\n",
      "Episode 1165. Average reward for episode -200.000000.  Total average reward -197.000000.\n",
      "Episode 1170. Average reward for episode -200.000000.  Total average reward -197.000000.\n",
      "Episode 1175. Average reward for episode -200.000000.  Total average reward -197.000000.\n",
      "Episode 1180. Average reward for episode -200.000000.  Total average reward -197.000000.\n",
      "Episode 1185. Average reward for episode -200.000000.  Total average reward -197.000000.\n",
      "Episode 1190. Average reward for episode -200.000000.  Total average reward -197.000000.\n",
      "Episode 1195. Average reward for episode -200.000000.  Total average reward -197.000000.\n",
      "Episode 1200. Average reward for episode -200.000000.  Total average reward -197.000000.\n",
      "Episode 1205. Average reward for episode -200.000000.  Total average reward -197.000000.\n",
      "Episode 1210. Average reward for episode -200.000000.  Total average reward -197.000000.\n",
      "Episode 1215. Average reward for episode -200.000000.  Total average reward -197.000000.\n",
      "Episode 1220. Average reward for episode -200.000000.  Total average reward -197.000000.\n",
      "Episode 1225. Average reward for episode -200.000000.  Total average reward -197.000000.\n",
      "Episode 1230. Average reward for episode -200.000000.  Total average reward -197.000000.\n",
      "Episode 1235. Average reward for episode -200.000000.  Total average reward -197.000000.\n",
      "Episode 1240. Average reward for episode -200.000000.  Total average reward -197.000000.\n",
      "Episode 1245. Average reward for episode -200.000000.  Total average reward -197.000000.\n",
      "Episode 1250. Average reward for episode -200.000000.  Total average reward -197.000000.\n",
      "Episode 1255. Average reward for episode -200.000000.  Total average reward -197.000000.\n",
      "Episode 1260. Average reward for episode -200.000000.  Total average reward -197.000000.\n",
      "Episode 1265. Average reward for episode -200.000000.  Total average reward -197.000000.\n",
      "Episode 1270. Average reward for episode -200.000000.  Total average reward -197.000000.\n",
      "Episode 1275. Average reward for episode -200.000000.  Total average reward -197.000000.\n",
      "Episode 1280. Average reward for episode -200.000000.  Total average reward -197.000000.\n",
      "Episode 1285. Average reward for episode -200.000000.  Total average reward -197.000000.\n",
      "Episode 1290. Average reward for episode -200.000000.  Total average reward -197.000000.\n",
      "Episode 1295. Average reward for episode -200.000000.  Total average reward -198.000000.\n",
      "Episode 1300. Average reward for episode -200.000000.  Total average reward -198.000000.\n",
      "Episode 1305. Average reward for episode -200.000000.  Total average reward -198.000000.\n",
      "Episode 1310. Average reward for episode -200.000000.  Total average reward -198.000000.\n",
      "Episode 1315. Average reward for episode -200.000000.  Total average reward -198.000000.\n",
      "Episode 1320. Average reward for episode -200.000000.  Total average reward -198.000000.\n",
      "Episode 1325. Average reward for episode -200.000000.  Total average reward -198.000000.\n",
      "Episode 1330. Average reward for episode -200.000000.  Total average reward -198.000000.\n",
      "Episode 1335. Average reward for episode -200.000000.  Total average reward -198.000000.\n",
      "Episode 1340. Average reward for episode -200.000000.  Total average reward -198.000000.\n",
      "Episode 1345. Average reward for episode -200.000000.  Total average reward -198.000000.\n",
      "Episode 1350. Average reward for episode -200.000000.  Total average reward -198.000000.\n",
      "Episode 1355. Average reward for episode -200.000000.  Total average reward -198.000000.\n",
      "Episode 1360. Average reward for episode -200.000000.  Total average reward -198.000000.\n",
      "Episode 1365. Average reward for episode -200.000000.  Total average reward -198.000000.\n",
      "Episode 1370. Average reward for episode -200.000000.  Total average reward -198.000000.\n",
      "Episode 1375. Average reward for episode -200.000000.  Total average reward -198.000000.\n",
      "Episode 1380. Average reward for episode -200.000000.  Total average reward -198.000000.\n",
      "Episode 1385. Average reward for episode -200.000000.  Total average reward -198.000000.\n",
      "Episode 1390. Average reward for episode -200.000000.  Total average reward -198.000000.\n",
      "Episode 1395. Average reward for episode -200.000000.  Total average reward -198.000000.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-eeea06d1e769>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# Run the policy network and get an action to take.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mtfprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobability\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mobservations\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtfprob\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "xs,hs,dlogps,drs,ys,tfps = [],[],[],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 1\n",
    "total_episodes = 10000\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    rendering = False\n",
    "    sess.run(init)\n",
    "    observation = env.reset() # Obtain an initial observation of the environment\n",
    "\n",
    "    # Reset the gradient placeholder. We will collect gradients in \n",
    "    # gradBuffer until we are ready to update our policy network. \n",
    "    gradBuffer = sess.run(tvars)\n",
    "    for ix,grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "    \n",
    "    while episode_number <= total_episodes:\n",
    "        \n",
    "        # Rendering the environment slows things down, \n",
    "        # so let's only look at it once our agent is doing a good job.\n",
    "        if reward_sum/batch_size > 150 or rendering == True : \n",
    "            env.render()\n",
    "            rendering = True\n",
    "            \n",
    "        # Make sure the observation is in a shape the network can handle.\n",
    "        x = np.reshape(observation,[1,D])\n",
    "        \n",
    "        # Run the policy network and get an action to take. \n",
    "        tfprob = sess.run(probability,feed_dict={observations: x})\n",
    "        action = 1 if np.random.uniform() < tfprob else 0\n",
    "        \n",
    "        xs.append(x) # observation\n",
    "        y = 1 if action == 0 else 0 # a \"fake label\"\n",
    "        ys.append(y)\n",
    "\n",
    "        # step the environment and get new measurements\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        reward_sum += reward\n",
    "\n",
    "        drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "        if done: \n",
    "            episode_number += 1\n",
    "            # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "            epx = np.vstack(xs)\n",
    "            epy = np.vstack(ys)\n",
    "            epr = np.vstack(drs)\n",
    "            tfp = tfps\n",
    "            xs,hs,dlogps,drs,ys,tfps = [],[],[],[],[],[] # reset array memory\n",
    "\n",
    "            # compute the discounted reward backwards through time\n",
    "            discounted_epr = discount_rewards(epr)\n",
    "            # size the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "            discounted_epr -= np.mean(discounted_epr)\n",
    "            discounted_epr //= np.std(discounted_epr)\n",
    "            \n",
    "            # Get the gradient for this episode, and save it in the gradBuffer\n",
    "            tGrad = sess.run(newGrads,feed_dict={observations: epx, input_y: epy, advantages: discounted_epr})\n",
    "            for ix,grad in enumerate(tGrad):\n",
    "                gradBuffer[ix] += grad\n",
    "                \n",
    "            # If we have completed enough episodes, then update the policy network with our gradients.\n",
    "            if episode_number % batch_size == 0: \n",
    "                sess.run(updateGrads,feed_dict={W1Grad: gradBuffer[0],W2Grad:gradBuffer[1]})\n",
    "                for ix,grad in enumerate(gradBuffer):\n",
    "                    gradBuffer[ix] = grad * 0\n",
    "                \n",
    "                # Give a summary of how well our network is doing for each batch of episodes.\n",
    "                running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "                print('Episode %i. Average reward for episode %f.  Total average reward %f.' % (episode_number, reward_sum//batch_size, running_reward//batch_size))\n",
    "                \n",
    "                if reward_sum//batch_size > 200: \n",
    "                    print(\"Task solved in\",episode_number,'episodes!')\n",
    "                    break\n",
    "                    \n",
    "                reward_sum = 0\n",
    "            \n",
    "            observation = env.reset()\n",
    "        \n",
    "print(episode_number,'Episodes completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the network not only does much better than random actions, but achieves the goal of 200 points per episode, thus solving the task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
