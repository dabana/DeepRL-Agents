{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Reinforcement Learning with Tensorflow: Part 3 - Model-Based RL\n",
    "In this iPython notebook we implement a policy and model network which work in tandem to solve the CartPole reinforcement learning problem. To learn more, read here: https://medium.com/p/9a6fe0cce99\n",
    "\n",
    "For more reinforcment learning tutorials, see:\n",
    "https://github.com/awjuliani/DeepRL-Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading libraries and starting CartPole environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ModuleNotFoundError:\n",
    "    import pickle\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.version_info.major > 2:\n",
    "    xrange = range\n",
    "del sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "H = 8 # number of hidden layer neurons\n",
    "learning_rate = 1e-2\n",
    "gamma = 0.99 # discount factor for reward\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "resume = False # resume from previous checkpoint?\n",
    "\n",
    "model_bs = 3 # Batch size when learning from model\n",
    "real_bs = 3 # Batch size when learning from real environment\n",
    "\n",
    "# model initialization\n",
    "D = 4 # input dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "observations = tf.placeholder(tf.float32, [None,4] , name=\"input_x\")\n",
    "W1 = tf.get_variable(\"W1\", shape=[4, H],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "layer1 = tf.nn.relu(tf.matmul(observations,W1))\n",
    "W2 = tf.get_variable(\"W2\", shape=[H, 1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "score = tf.matmul(layer1,W2)\n",
    "probability = tf.nn.sigmoid(score)\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "input_y = tf.placeholder(tf.float32,[None,1], name=\"input_y\")\n",
    "advantages = tf.placeholder(tf.float32,name=\"reward_signal\")\n",
    "adam = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "W1Grad = tf.placeholder(tf.float32,name=\"batch_grad1\")\n",
    "W2Grad = tf.placeholder(tf.float32,name=\"batch_grad2\")\n",
    "batchGrad = [W1Grad,W2Grad]\n",
    "loglik = tf.log(input_y*(input_y - probability) + (1 - input_y)*(input_y + probability))\n",
    "loss = -tf.reduce_mean(loglik * advantages) \n",
    "newGrads = tf.gradients(loss,tvars)\n",
    "updateGrads = adam.apply_gradients(zip(batchGrad,tvars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Network\n",
    "Here we implement a multi-layer neural network that predicts the next observation, reward, and done state from a current state and action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mH = 256 # model layer size\n",
    "\n",
    "input_data = tf.placeholder(tf.float32, [None, 5])\n",
    "with tf.variable_scope('rnnlm'):\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [mH, 50])\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [50])\n",
    "\n",
    "previous_state = tf.placeholder(tf.float32, [None,5] , name=\"previous_state\")\n",
    "W1M = tf.get_variable(\"W1M\", shape=[5, mH],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "B1M = tf.Variable(tf.zeros([mH]),name=\"B1M\")\n",
    "layer1M = tf.nn.relu(tf.matmul(previous_state,W1M) + B1M)\n",
    "W2M = tf.get_variable(\"W2M\", shape=[mH, mH],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "B2M = tf.Variable(tf.zeros([mH]),name=\"B2M\")\n",
    "layer2M = tf.nn.relu(tf.matmul(layer1M,W2M) + B2M)\n",
    "wO = tf.get_variable(\"wO\", shape=[mH, 4],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "wR = tf.get_variable(\"wR\", shape=[mH, 1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "wD = tf.get_variable(\"wD\", shape=[mH, 1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "bO = tf.Variable(tf.zeros([4]),name=\"bO\")\n",
    "bR = tf.Variable(tf.zeros([1]),name=\"bR\")\n",
    "bD = tf.Variable(tf.ones([1]),name=\"bD\")\n",
    "\n",
    "\n",
    "predicted_observation = tf.matmul(layer2M,wO,name=\"predicted_observation\") + bO\n",
    "predicted_reward = tf.matmul(layer2M,wR,name=\"predicted_reward\") + bR\n",
    "predicted_done = tf.sigmoid(tf.matmul(layer2M,wD,name=\"predicted_done\") + bD)\n",
    "\n",
    "true_observation = tf.placeholder(tf.float32,[None,4],name=\"true_observation\")\n",
    "true_reward = tf.placeholder(tf.float32,[None,1],name=\"true_reward\")\n",
    "true_done = tf.placeholder(tf.float32,[None,1],name=\"true_done\")\n",
    "\n",
    "\n",
    "predicted_state = tf.concat([predicted_observation,predicted_reward,predicted_done],1)\n",
    "\n",
    "observation_loss = tf.square(true_observation - predicted_observation)\n",
    "\n",
    "reward_loss = tf.square(true_reward - predicted_reward)\n",
    "\n",
    "done_loss = tf.multiply(predicted_done, true_done) + tf.multiply(1-predicted_done, 1-true_done)\n",
    "done_loss = -tf.log(done_loss)\n",
    "\n",
    "model_loss = tf.reduce_mean(observation_loss + done_loss + reward_loss)\n",
    "\n",
    "modelAdam = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "updateModel = modelAdam.minimize(model_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resetGradBuffer(gradBuffer):\n",
    "    for ix,grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "    return gradBuffer\n",
    "        \n",
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "\n",
    "# This function uses our model to produce a new state when given a previous state and action\n",
    "def stepModel(sess, xs, action):\n",
    "    toFeed = np.reshape(np.hstack([xs[-1][0],np.array(action)]),[1,5])\n",
    "    myPredict = sess.run([predicted_state],feed_dict={previous_state: toFeed})\n",
    "    reward = myPredict[0][:,4]\n",
    "    observation = myPredict[0][:,0:4]\n",
    "    observation[:,0] = np.clip(observation[:,0],-2.4,2.4)\n",
    "    observation[:,2] = np.clip(observation[:,2],-0.4,0.4)\n",
    "    doneP = np.clip(myPredict[0][:,5],0,1)\n",
    "    if doneP > 0.1 or len(xs)>= 300:\n",
    "        done = True\n",
    "    else:\n",
    "        done = False\n",
    "    return observation, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Policy and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 4.000000. Reward 28.666667. action: 1.000000. mean reward 28.666667.\n",
      "World Perf: Episode 7.000000. Reward 24.333333. action: 0.000000. mean reward 28.623333.\n",
      "World Perf: Episode 10.000000. Reward 12.333333. action: 1.000000. mean reward 28.460433.\n",
      "World Perf: Episode 13.000000. Reward 27.666667. action: 1.000000. mean reward 28.452496.\n",
      "World Perf: Episode 16.000000. Reward 27.000000. action: 1.000000. mean reward 28.437971.\n",
      "World Perf: Episode 19.000000. Reward 22.333333. action: 1.000000. mean reward 28.376924.\n",
      "World Perf: Episode 22.000000. Reward 17.666667. action: 0.000000. mean reward 28.269822.\n",
      "World Perf: Episode 25.000000. Reward 23.000000. action: 0.000000. mean reward 28.217124.\n",
      "World Perf: Episode 28.000000. Reward 32.666667. action: 1.000000. mean reward 28.261619.\n",
      "World Perf: Episode 31.000000. Reward 16.666667. action: 0.000000. mean reward 28.145669.\n",
      "World Perf: Episode 34.000000. Reward 18.333333. action: 0.000000. mean reward 28.047546.\n",
      "World Perf: Episode 37.000000. Reward 22.000000. action: 0.000000. mean reward 27.987071.\n",
      "World Perf: Episode 40.000000. Reward 27.000000. action: 1.000000. mean reward 27.977200.\n",
      "World Perf: Episode 43.000000. Reward 19.000000. action: 1.000000. mean reward 27.887428.\n",
      "World Perf: Episode 46.000000. Reward 14.666667. action: 0.000000. mean reward 27.755220.\n",
      "World Perf: Episode 49.000000. Reward 19.000000. action: 1.000000. mean reward 27.667668.\n",
      "World Perf: Episode 52.000000. Reward 15.666667. action: 1.000000. mean reward 27.547658.\n",
      "World Perf: Episode 55.000000. Reward 24.666667. action: 0.000000. mean reward 27.518848.\n",
      "World Perf: Episode 58.000000. Reward 23.666667. action: 0.000000. mean reward 27.480326.\n",
      "World Perf: Episode 61.000000. Reward 20.666667. action: 0.000000. mean reward 27.412190.\n",
      "World Perf: Episode 64.000000. Reward 20.000000. action: 1.000000. mean reward 27.338068.\n",
      "World Perf: Episode 67.000000. Reward 21.666667. action: 1.000000. mean reward 27.281354.\n",
      "World Perf: Episode 70.000000. Reward 21.666667. action: 0.000000. mean reward 27.225207.\n",
      "World Perf: Episode 73.000000. Reward 12.333333. action: 1.000000. mean reward 27.076288.\n",
      "World Perf: Episode 76.000000. Reward 30.666667. action: 1.000000. mean reward 27.112192.\n",
      "World Perf: Episode 79.000000. Reward 14.333333. action: 0.000000. mean reward 26.984403.\n",
      "World Perf: Episode 82.000000. Reward 30.333333. action: 0.000000. mean reward 27.017893.\n",
      "World Perf: Episode 85.000000. Reward 23.333333. action: 0.000000. mean reward 26.981047.\n",
      "World Perf: Episode 88.000000. Reward 21.333333. action: 1.000000. mean reward 26.924570.\n",
      "World Perf: Episode 91.000000. Reward 15.333333. action: 1.000000. mean reward 26.808658.\n",
      "World Perf: Episode 94.000000. Reward 27.000000. action: 0.000000. mean reward 26.810571.\n",
      "World Perf: Episode 97.000000. Reward 17.000000. action: 1.000000. mean reward 26.712465.\n",
      "World Perf: Episode 100.000000. Reward 21.666667. action: 1.000000. mean reward 26.662007.\n",
      "World Perf: Episode 103.000000. Reward 16.000000. action: 1.000000. mean reward 26.555387.\n",
      "World Perf: Episode 106.000000. Reward 40.666667. action: 0.000000. mean reward 29.556101.\n",
      "World Perf: Episode 109.000000. Reward 48.000000. action: 1.000000. mean reward 32.467518.\n",
      "World Perf: Episode 112.000000. Reward 28.333333. action: 0.000000. mean reward 32.307762.\n",
      "World Perf: Episode 115.000000. Reward 36.000000. action: 1.000000. mean reward 32.098881.\n",
      "World Perf: Episode 118.000000. Reward 12.000000. action: 0.000000. mean reward 31.639549.\n",
      "World Perf: Episode 121.000000. Reward 13.000000. action: 1.000000. mean reward 31.212769.\n",
      "World Perf: Episode 124.000000. Reward 30.666667. action: 0.000000. mean reward 33.664883.\n",
      "World Perf: Episode 127.000000. Reward 30.000000. action: 0.000000. mean reward 36.848076.\n",
      "World Perf: Episode 130.000000. Reward 30.666667. action: 0.000000. mean reward 36.726444.\n",
      "World Perf: Episode 133.000000. Reward 28.666667. action: 1.000000. mean reward 36.382881.\n",
      "World Perf: Episode 136.000000. Reward 24.000000. action: 1.000000. mean reward 36.055073.\n",
      "World Perf: Episode 139.000000. Reward 20.000000. action: 0.000000. mean reward 35.809948.\n",
      "World Perf: Episode 142.000000. Reward 50.333333. action: 0.000000. mean reward 38.575211.\n",
      "World Perf: Episode 145.000000. Reward 19.333333. action: 1.000000. mean reward 40.832218.\n",
      "World Perf: Episode 148.000000. Reward 17.666667. action: 0.000000. mean reward 40.355572.\n",
      "World Perf: Episode 151.000000. Reward 27.333333. action: 1.000000. mean reward 40.122936.\n",
      "World Perf: Episode 154.000000. Reward 19.666667. action: 0.000000. mean reward 39.676640.\n",
      "World Perf: Episode 157.000000. Reward 18.333333. action: 0.000000. mean reward 39.169407.\n",
      "World Perf: Episode 160.000000. Reward 21.333333. action: 1.000000. mean reward 40.650738.\n",
      "World Perf: Episode 163.000000. Reward 35.666667. action: 0.000000. mean reward 40.256493.\n",
      "World Perf: Episode 166.000000. Reward 24.000000. action: 1.000000. mean reward 39.771351.\n",
      "World Perf: Episode 169.000000. Reward 18.000000. action: 0.000000. mean reward 39.283276.\n",
      "World Perf: Episode 172.000000. Reward 76.000000. action: 1.000000. mean reward 39.580391.\n",
      "World Perf: Episode 175.000000. Reward 31.666667. action: 0.000000. mean reward 39.292961.\n",
      "World Perf: Episode 178.000000. Reward 18.333333. action: 1.000000. mean reward 41.823124.\n",
      "World Perf: Episode 181.000000. Reward 32.000000. action: 0.000000. mean reward 41.371937.\n",
      "World Perf: Episode 184.000000. Reward 51.333333. action: 0.000000. mean reward 44.835567.\n",
      "World Perf: Episode 187.000000. Reward 22.000000. action: 1.000000. mean reward 44.231602.\n",
      "World Perf: Episode 190.000000. Reward 42.333333. action: 1.000000. mean reward 43.941757.\n",
      "World Perf: Episode 193.000000. Reward 28.666667. action: 0.000000. mean reward 44.437851.\n",
      "World Perf: Episode 196.000000. Reward 38.000000. action: 1.000000. mean reward 47.055408.\n",
      "World Perf: Episode 199.000000. Reward 59.666667. action: 0.000000. mean reward 48.330944.\n",
      "World Perf: Episode 202.000000. Reward 54.333333. action: 0.000000. mean reward 48.060574.\n",
      "World Perf: Episode 205.000000. Reward 21.333333. action: 0.000000. mean reward 49.691738.\n",
      "World Perf: Episode 208.000000. Reward 34.666667. action: 0.000000. mean reward 49.196518.\n",
      "World Perf: Episode 211.000000. Reward 32.333333. action: 0.000000. mean reward 48.645069.\n",
      "World Perf: Episode 214.000000. Reward 18.333333. action: 1.000000. mean reward 48.148235.\n",
      "World Perf: Episode 217.000000. Reward 49.000000. action: 0.000000. mean reward 47.816547.\n",
      "World Perf: Episode 220.000000. Reward 30.666667. action: 0.000000. mean reward 50.158051.\n",
      "World Perf: Episode 223.000000. Reward 19.333333. action: 0.000000. mean reward 49.485310.\n",
      "World Perf: Episode 226.000000. Reward 22.000000. action: 1.000000. mean reward 48.777225.\n",
      "World Perf: Episode 229.000000. Reward 21.666667. action: 0.000000. mean reward 48.334843.\n",
      "World Perf: Episode 232.000000. Reward 26.333333. action: 0.000000. mean reward 47.686878.\n",
      "World Perf: Episode 235.000000. Reward 28.333333. action: 0.000000. mean reward 47.218868.\n",
      "World Perf: Episode 238.000000. Reward 29.333333. action: 1.000000. mean reward 46.915104.\n",
      "World Perf: Episode 241.000000. Reward 37.666667. action: 0.000000. mean reward 47.043961.\n",
      "World Perf: Episode 244.000000. Reward 33.666667. action: 1.000000. mean reward 48.396820.\n",
      "World Perf: Episode 247.000000. Reward 24.000000. action: 0.000000. mean reward 48.102097.\n",
      "World Perf: Episode 250.000000. Reward 22.666667. action: 1.000000. mean reward 48.363667.\n",
      "World Perf: Episode 253.000000. Reward 53.000000. action: 1.000000. mean reward 50.867786.\n",
      "World Perf: Episode 256.000000. Reward 37.333333. action: 1.000000. mean reward 50.539501.\n",
      "World Perf: Episode 259.000000. Reward 27.000000. action: 1.000000. mean reward 51.065052.\n",
      "World Perf: Episode 262.000000. Reward 41.000000. action: 1.000000. mean reward 50.648891.\n",
      "World Perf: Episode 265.000000. Reward 71.000000. action: 0.000000. mean reward 50.407761.\n",
      "World Perf: Episode 268.000000. Reward 50.000000. action: 1.000000. mean reward 50.017685.\n",
      "World Perf: Episode 271.000000. Reward 37.000000. action: 0.000000. mean reward 55.995914.\n",
      "World Perf: Episode 274.000000. Reward 16.333333. action: 1.000000. mean reward 55.588352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 277.000000. Reward 27.333333. action: 1.000000. mean reward 59.312668.\n",
      "World Perf: Episode 280.000000. Reward 33.333333. action: 0.000000. mean reward 62.661011.\n",
      "World Perf: Episode 283.000000. Reward 29.333333. action: 0.000000. mean reward 64.371490.\n",
      "World Perf: Episode 286.000000. Reward 33.333333. action: 0.000000. mean reward 63.992702.\n",
      "World Perf: Episode 289.000000. Reward 31.666667. action: 0.000000. mean reward 63.200901.\n",
      "World Perf: Episode 292.000000. Reward 82.333333. action: 0.000000. mean reward 65.786171.\n",
      "World Perf: Episode 295.000000. Reward 66.000000. action: 0.000000. mean reward 67.999435.\n",
      "World Perf: Episode 298.000000. Reward 53.000000. action: 1.000000. mean reward 70.185326.\n",
      "World Perf: Episode 301.000000. Reward 51.333333. action: 0.000000. mean reward 69.636192.\n",
      "World Perf: Episode 304.000000. Reward 49.000000. action: 0.000000. mean reward 72.427109.\n",
      "World Perf: Episode 307.000000. Reward 74.000000. action: 0.000000. mean reward 74.497604.\n",
      "World Perf: Episode 310.000000. Reward 30.333333. action: 0.000000. mean reward 76.119293.\n",
      "World Perf: Episode 313.000000. Reward 63.333333. action: 0.000000. mean reward 75.816872.\n",
      "World Perf: Episode 316.000000. Reward 42.666667. action: 0.000000. mean reward 75.623444.\n",
      "World Perf: Episode 319.000000. Reward 42.000000. action: 1.000000. mean reward 75.837029.\n",
      "World Perf: Episode 322.000000. Reward 24.333333. action: 1.000000. mean reward 74.704643.\n",
      "World Perf: Episode 325.000000. Reward 34.666667. action: 0.000000. mean reward 73.686302.\n",
      "World Perf: Episode 328.000000. Reward 59.000000. action: 0.000000. mean reward 72.935928.\n",
      "World Perf: Episode 331.000000. Reward 61.666667. action: 0.000000. mean reward 73.693016.\n",
      "World Perf: Episode 334.000000. Reward 41.333333. action: 1.000000. mean reward 72.790749.\n",
      "World Perf: Episode 337.000000. Reward 33.000000. action: 0.000000. mean reward 71.799950.\n",
      "World Perf: Episode 340.000000. Reward 62.000000. action: 0.000000. mean reward 73.280052.\n",
      "World Perf: Episode 343.000000. Reward 44.000000. action: 1.000000. mean reward 72.321968.\n",
      "World Perf: Episode 346.000000. Reward 52.333333. action: 1.000000. mean reward 71.495811.\n",
      "World Perf: Episode 349.000000. Reward 55.666667. action: 0.000000. mean reward 71.680885.\n",
      "World Perf: Episode 352.000000. Reward 43.000000. action: 0.000000. mean reward 74.387062.\n",
      "World Perf: Episode 355.000000. Reward 52.333333. action: 0.000000. mean reward 75.766380.\n",
      "World Perf: Episode 358.000000. Reward 56.666667. action: 0.000000. mean reward 74.906616.\n",
      "World Perf: Episode 361.000000. Reward 44.666667. action: 1.000000. mean reward 73.992485.\n",
      "World Perf: Episode 364.000000. Reward 40.666667. action: 1.000000. mean reward 75.328896.\n",
      "World Perf: Episode 367.000000. Reward 64.333333. action: 1.000000. mean reward 74.560020.\n",
      "World Perf: Episode 370.000000. Reward 58.666667. action: 1.000000. mean reward 76.652748.\n",
      "World Perf: Episode 373.000000. Reward 67.000000. action: 0.000000. mean reward 76.104088.\n",
      "World Perf: Episode 376.000000. Reward 42.333333. action: 0.000000. mean reward 75.278450.\n",
      "World Perf: Episode 379.000000. Reward 47.000000. action: 0.000000. mean reward 74.329201.\n",
      "World Perf: Episode 382.000000. Reward 70.000000. action: 1.000000. mean reward 73.774498.\n",
      "World Perf: Episode 385.000000. Reward 49.666667. action: 1.000000. mean reward 73.580452.\n",
      "World Perf: Episode 388.000000. Reward 53.000000. action: 1.000000. mean reward 72.771057.\n",
      "World Perf: Episode 391.000000. Reward 58.666667. action: 1.000000. mean reward 72.087677.\n",
      "World Perf: Episode 394.000000. Reward 31.000000. action: 0.000000. mean reward 71.667252.\n",
      "World Perf: Episode 397.000000. Reward 48.666667. action: 1.000000. mean reward 70.968231.\n",
      "World Perf: Episode 400.000000. Reward 55.666667. action: 1.000000. mean reward 70.329353.\n",
      "World Perf: Episode 403.000000. Reward 45.666667. action: 0.000000. mean reward 69.518959.\n",
      "World Perf: Episode 406.000000. Reward 87.333333. action: 1.000000. mean reward 69.338432.\n",
      "World Perf: Episode 409.000000. Reward 106.333333. action: 1.000000. mean reward 69.457397.\n",
      "World Perf: Episode 412.000000. Reward 61.000000. action: 1.000000. mean reward 69.009102.\n",
      "World Perf: Episode 415.000000. Reward 113.333333. action: 0.000000. mean reward 68.964661.\n",
      "World Perf: Episode 418.000000. Reward 86.333333. action: 1.000000. mean reward 71.655479.\n",
      "World Perf: Episode 421.000000. Reward 67.333333. action: 1.000000. mean reward 71.126579.\n",
      "World Perf: Episode 424.000000. Reward 83.666667. action: 1.000000. mean reward 70.667786.\n",
      "World Perf: Episode 427.000000. Reward 39.666667. action: 0.000000. mean reward 69.965950.\n",
      "World Perf: Episode 430.000000. Reward 72.333333. action: 1.000000. mean reward 69.735840.\n",
      "World Perf: Episode 433.000000. Reward 75.666667. action: 0.000000. mean reward 69.256660.\n"
     ]
    }
   ],
   "source": [
    "xs,drs,ys,ds = [],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 1\n",
    "real_episodes = 1\n",
    "init = tf.global_variables_initializer()\n",
    "batch_size = real_bs\n",
    "\n",
    "drawFromModel = False # When set to True, will use model for observations\n",
    "trainTheModel = True # Whether to train the model\n",
    "trainThePolicy = False # Whether to train the policy\n",
    "switch_point = 1\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    rendering = False\n",
    "    sess.run(init)\n",
    "    observation = env.reset()\n",
    "    x = observation\n",
    "    gradBuffer = sess.run(tvars)\n",
    "    gradBuffer = resetGradBuffer(gradBuffer)\n",
    "    \n",
    "    while episode_number <= 5000:\n",
    "        # Start displaying environment once performance is acceptably high.\n",
    "        if (reward_sum/batch_size > 150 and drawFromModel == False) or rendering == True : \n",
    "            env.render()\n",
    "            rendering = True\n",
    "            \n",
    "        x = np.reshape(observation,[1,4])\n",
    "\n",
    "        tfprob = sess.run(probability,feed_dict={observations: x})\n",
    "        action = 1 if np.random.uniform() < tfprob else 0\n",
    "\n",
    "        # record various intermediates (needed later for backprop)\n",
    "        xs.append(x) \n",
    "        y = 1 if action == 0 else 0 \n",
    "        ys.append(y)\n",
    "        \n",
    "        # step the  model or real environment and get new measurements\n",
    "        if drawFromModel == False:\n",
    "            observation, reward, done, info = env.step(action)\n",
    "        else:\n",
    "            observation, reward, done = stepModel(sess,xs,action)\n",
    "                \n",
    "        reward_sum += reward\n",
    "        \n",
    "        ds.append(done*1)\n",
    "        drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "        if done: \n",
    "            \n",
    "            if drawFromModel == False: \n",
    "                real_episodes += 1\n",
    "            episode_number += 1\n",
    "\n",
    "            # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "            epx = np.vstack(xs)\n",
    "            epy = np.vstack(ys)\n",
    "            epr = np.vstack(drs)\n",
    "            epd = np.vstack(ds)\n",
    "            xs,drs,ys,ds = [],[],[],[] # reset array memory\n",
    "            \n",
    "            if trainTheModel == True:\n",
    "                actions = np.array([np.abs(y-1) for y in epy][:-1])\n",
    "                state_prevs = epx[:-1,:]\n",
    "                state_prevs = np.hstack([state_prevs,actions])\n",
    "                state_nexts = epx[1:,:]\n",
    "                rewards = np.array(epr[1:,:])\n",
    "                dones = np.array(epd[1:,:])\n",
    "                state_nextsAll = np.hstack([state_nexts,rewards,dones])\n",
    "\n",
    "                feed_dict={previous_state: state_prevs, true_observation: state_nexts,true_done:dones,true_reward:rewards}\n",
    "                loss,pState,_ = sess.run([model_loss,predicted_state,updateModel],feed_dict)\n",
    "            if trainThePolicy == True:\n",
    "                discounted_epr = discount_rewards(epr).astype('float32')\n",
    "                discounted_epr -= np.mean(discounted_epr)\n",
    "                discounted_epr /= np.std(discounted_epr)\n",
    "                tGrad = sess.run(newGrads,feed_dict={observations: epx, input_y: epy, advantages: discounted_epr})\n",
    "                \n",
    "                # If gradients becom too large, end training process\n",
    "                if np.sum(tGrad[0] == tGrad[0]) == 0:\n",
    "                    break\n",
    "                for ix,grad in enumerate(tGrad):\n",
    "                    gradBuffer[ix] += grad\n",
    "                \n",
    "            if switch_point + batch_size == episode_number: \n",
    "                switch_point = episode_number\n",
    "                if trainThePolicy == True:\n",
    "                    sess.run(updateGrads,feed_dict={W1Grad: gradBuffer[0],W2Grad:gradBuffer[1]})\n",
    "                    gradBuffer = resetGradBuffer(gradBuffer)\n",
    "\n",
    "                running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "                if drawFromModel == False:\n",
    "                    print('World Perf: Episode %f. Reward %f. action: %f. mean reward %f.' % (real_episodes,reward_sum/real_bs,action, running_reward/real_bs))\n",
    "                    if reward_sum/batch_size > 200:\n",
    "                        break\n",
    "                reward_sum = 0\n",
    "\n",
    "                # Once the model has been trained on 100 episodes, we start alternating between training the policy\n",
    "                # from the model and training the model from the real environment.\n",
    "                if episode_number > 100:\n",
    "                    drawFromModel = not drawFromModel\n",
    "                    trainTheModel = not trainTheModel\n",
    "                    trainThePolicy = not trainThePolicy\n",
    "            \n",
    "            if drawFromModel == True:\n",
    "                observation = np.random.uniform(-0.1,0.1,[4]) # Generate reasonable starting point\n",
    "                batch_size = model_bs\n",
    "            else:\n",
    "                observation = env.reset()\n",
    "                batch_size = real_bs\n",
    "                \n",
    "print(real_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking model representation\n",
    "Here we can examine how well the model is able to approximate the true environment after training. The green line indicates the real environment, and the blue indicates model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 12))\n",
    "for i in range(6):\n",
    "    plt.subplot(6, 2, 2*i + 1)\n",
    "    plt.plot(pState[:,i])\n",
    "    plt.subplot(6,2,2*i+1)\n",
    "    plt.plot(state_nextsAll[:,i])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
